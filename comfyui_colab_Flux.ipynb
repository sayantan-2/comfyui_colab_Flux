{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayantan-2/comfyui_colab_Flux/blob/main/comfyui_colab_Flux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the [ComfyUI](https://github.com/comfyanonymous/ComfyUI) repo and install the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bbbbbbbbbb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd ComfyUI\n",
        "%pip install -q xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git clone the [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) repo and follow the instructions."
      ],
      "metadata": {
        "id": "rBW-zqXfzsYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dvla7XiuVpN"
      },
      "outputs": [],
      "source": [
        "%cd custom_nodes\n",
        "%pip install --upgrade gguf\n",
        "!rm -rf ComfyUI-GGUF\n",
        "!git clone https://github.com/city96/ComfyUI-GGUF\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download the vae and dual text encoder needed for FLUX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb9xz1ViENoO"
      },
      "outputs": [],
      "source": [
        "!wget -q --show-progress https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors -P ./models/text_encoders/\n",
        "!wget -q --show-progress https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors -P ./models/text_encoders/\n",
        "!wget -q --show-progress https://huggingface.co/cocktailpeanut/xulf-dev/resolve/main/ae.safetensors -P ./models/vae/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWglWy21uVpP"
      },
      "source": [
        "Download the **FLUX.1-dev-gguf** models.  \n",
        "The **FP8 version** runs smoothly on the free tier (without LoRA), but if you want to use it with a LoRA, it requires more resources than the free Colab tier provides.  \n",
        "In that case, you can opt for the gguf quantized versions instead. For more quantized models, check [this page](https://huggingface.co/collections/QuantStack/flux-ggufs-68264cfc663d50c418940b30).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojrmju_XuVpQ"
      },
      "outputs": [],
      "source": [
        "# Download any one\n",
        "\n",
        "# fp8 - needs more than free tier\n",
        "# !wget -O ./models/checkpoints/flux1-dev-fp8.safetensors https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors\n",
        "\n",
        "# flux1-dev\n",
        "!wget -q --show-progress https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q6_K.gguf -P ./models/unet\n",
        "\n",
        "# flux1-kontext-dev\n",
        "# !wget -q --show-progress https://huggingface.co/QuantStack/FLUX.1-Kontext-dev-GGUF/resolve/main/flux1-kontext-dev-Q6_K.gguf -P ./models/unet\n",
        "\n",
        "# flux1-krea-dev\n",
        "# !wget -q --show-progress https://huggingface.co/QuantStack/FLUX.1-Krea-dev-GGUF/resolve/main/flux1-krea-dev-Q6_K.gguf -P ./models/unet\n",
        "\n",
        "# flux1-schnell\n",
        "# !wget -q --show-progress https://huggingface.co/city96/FLUX.1-schnell-gguf/resolve/main/flux1-schnell-Q6_K.gguf -P ./models/unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "47QdZgYl4EzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "import sys\n",
        "\n",
        "def iframe_thread(port):\n",
        "    while True:\n",
        "        time.sleep(0.5)\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        result = sock.connect_ex(('127.0.0.1', port))\n",
        "        if result == 0:\n",
        "            break\n",
        "        sock.close()\n",
        "\n",
        "    print(\"\\nComfyUI finished loading, trying to launch localtunnel...\\n\")\n",
        "    print(\"The password/endpoint IP for localtunnel is:\",\n",
        "          urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "\n",
        "    p = subprocess.Popen([\"lt\", \"--port\", str(port)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
        "\n",
        "    for line in iter(p.stdout.readline, ''):\n",
        "        sys.stdout.write(line)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "!python main.py --dont-print-server"
      ],
      "metadata": {
        "id": "h4B6fEps3VpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gggggggggg"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}